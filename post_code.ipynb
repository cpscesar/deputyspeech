{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading and saving the dataset with the information of the deputies\n",
    "URL = 'http://dadosabertos.camara.leg.br/arquivos/deputados/csv/deputados.csv'\n",
    "response = requests.get(URL)\n",
    "\n",
    "with open(\"deputados.csv\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the dataset with the information of the deputies\n",
    "dep = pd.read_csv('deputados.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save the speeches and the info about the deputies\n",
    "def get_speeches(database, checkpoint):\n",
    "    \"\"\"\n",
    "    Retrieve speeches from the given database.\n",
    "\n",
    "    Parameters:\n",
    "        database (pd.DataFrame): DataFrame containing information about the\n",
    "            deputies.\n",
    "        checkpoint (int): The interval at which to save the .csv file.\n",
    "    \"\"\"\n",
    "    # Creating the dictionary\n",
    "    data = {\n",
    "        'deputy_code': [],\n",
    "        'keywords': [],\n",
    "        'start_time': [],\n",
    "        'transcription': []\n",
    "    }\n",
    "\n",
    "    # Downloading the speeches\n",
    "    for uri in database['uri'].tolist():\n",
    "        try:\n",
    "            response = requests.get(f'{uri}/discursos?dataInicio=1988-01-01&dataFim=2022-08-01&ordenarPor=dataHoraInicio&ordem=ASC')\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Extracting the deputy code\n",
    "            deputy_code = re.sub(r'.', '', uri, count=52)\n",
    "\n",
    "            # Looping over the speeches\n",
    "            for i, speech in enumerate(response_data['dados']):\n",
    "                # Adding the deputy code to the dictionary\n",
    "                data['deputy_code'].append(deputy_code)\n",
    "\n",
    "                # Adding the keywords\n",
    "                data['keywords'].append(speech['keywords'])\n",
    "\n",
    "                # Adding the start time\n",
    "                data['start_time'].append(speech['dataHoraInicio'])\n",
    "\n",
    "                # Adding the transcription\n",
    "                data['transcription'].append(speech['transcricao'])\n",
    "\n",
    "                # Saving .csv checkpoint every checkpoint rows\n",
    "                if (i + 1) % checkpoint == 0:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(f'speeches{checkpoint}.csv', index=False)\n",
    "                    print(checkpoint)\n",
    "                    checkpoint += checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading speech for URI {uri}: {e}\")\n",
    "            continue\n",
    "    # Saving the data to a .csv file after the loop\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"speeches.csv\", index=False)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_deputy_info(database, checkpoint):\n",
    "    \"\"\" database: dataframe containing the code of the deputies - format of the CÃ¢mara dos Deputados API site\n",
    "        checkpoint: number of rows checkpoint to save\n",
    "    \"\"\"\n",
    "\n",
    "    current_checkpoint = checkpoint\n",
    "\n",
    "    # Creating the dictionary\n",
    "    info_dict = {'deputy_code':[], 'legislature_id':[], 'full_name':[], 'party_abbreviation':[], 'state_abbreviation':[], 'party_uri':[]}\n",
    "\n",
    "    # Download the information\n",
    "    for i, deputy_uri in enumerate(database['uri'].values.tolist()):\n",
    "        try:\n",
    "            response = requests.get(f'{deputy_uri}/')\n",
    "            data = response.json()\n",
    "\n",
    "            # Getting the code for each deputy\n",
    "            deputy_code = re.sub(r'.', '', deputy_uri, count = 52)\n",
    "\n",
    "            # Adding the deputy code to the dictionary\n",
    "            info_dict['deputy_code'].append(deputy_code)\n",
    "\n",
    "            # Saving the legislature_id\n",
    "            legislature_id = data['dados']['ultimoStatus']['idLegislatura']\n",
    "            info_dict['legislature_id'].append(legislature_id)\n",
    "\n",
    "            # Saving the full_name\n",
    "            full_name = data['dados']['nomeCivil']\n",
    "            info_dict['full_name'].append(full_name)\n",
    "\n",
    "            # Saving the party_abbreviation\n",
    "            party_abbreviation = data['dados']['ultimoStatus']['siglaPartido']\n",
    "            info_dict['party_abbreviation'].append(party_abbreviation)\n",
    "\n",
    "            # Saving the state_abbreviation\n",
    "            state_abbreviation = data['dados']['ultimoStatus']['siglaUf']\n",
    "            info_dict['state_abbreviation'].append(state_abbreviation)\n",
    "\n",
    "            # Saving the party_uri\n",
    "            party_uri = data['dados']['ultimoStatus']['uriPartido']\n",
    "            info_dict['party_uri'].append(party_uri)\n",
    "\n",
    "            # Saving .csv checkpoint every checkpoint rows\n",
    "            if (i + 1) % current_checkpoint == 0:\n",
    "                df = pd.DataFrame(info_dict)\n",
    "                df.to_csv(f'deputy_info_{current_checkpoint}.csv', index=False)\n",
    "                print(f\"Saved checkpoint of {current_checkpoint} rows\")\n",
    "                current_checkpoint += checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading speech for URI {deputy_uri}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Saving the data to a .csv file after the loop\n",
    "    df = pd.DataFrame(info_dict)\n",
    "    df.to_csv(\"deputy_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the functions\n",
    "get_speeches(database = dep, checkpoint = 1)\n",
    "get_deputy_info(database = dep, checkpoint = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening downloaded datasets to join them\n",
    "speeches = pd.read_csv('speeches.csv')\n",
    "deputy_info = pd.read_csv('deputy_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the code for each deputy\n",
    "deputy_codes = []\n",
    "for deputy_uri in dep['uri'].tolist():\n",
    "    deputy_code = re.sub(r'.', '', deputy_uri, count = 52)\n",
    "    deputy_codes.append(deputy_code)\n",
    "\n",
    "#Add the deputy codes to the database\n",
    "dep['deputy_code'] = deputy_codes\n",
    "\n",
    "#Change the format of the key variables for merging the databases\n",
    "speeches['deputy_code'] = speeches['deputy_code'].astype(int).astype(str)\n",
    "deputy_info['deputy_code'] = deputy_info['deputy_code'].astype(int).astype(str)\n",
    "\n",
    "#Merge the databases\n",
    "df1 = pd.merge(speeches, dep, on='deputy_code')\n",
    "df2 = pd.merge(df1, deputy_info, on='deputy_code')\n",
    "\n",
    "#Select the relevant variables\n",
    "df2 = df2[['deputy_code', 'keywords', 'start_time', 'transcription', 'full_name', 'siglaSexo', 'dataNascimento', 'ufNascimento', 'party_abbreviation', 'state_abbreviation']]\n",
    "\n",
    "#Remove rows that don't have any transcription\n",
    "df3 = df2[df2['transcription'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final dataset to a .csv file to build the graphs\n",
    "df3.to_csv(\"df_analyses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8c14ae9fbdef0c4cf732716ab3acd4fcf86544210da4d9aeb48e8decd64e9fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
